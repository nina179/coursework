{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Logistic Regression & Stochastic Gradient Descent\n",
    "\n",
    "**Due: October 24th at 11:59pm**\n",
    "\n",
    "***\n",
    "## Learning Goals\n",
    "\n",
    "The goals of this lab is to  implement the Logistic Regression classification model and the stochastic gradient descent algorithm. \n",
    "\n",
    "As with any programming assignment, you'll also be practicing and improving your general CS skills, like problem decomposition, algorithmic thinking, implementation and testing, language syntax, etc..  Here are some of the specific things you should learn while completing this assignment:\n",
    "\n",
    "\n",
    "* How to implement the Logistic Regression classification model\n",
    "* How to implement the Stochastic Gradient Descent optimization algorithm\n",
    "* How to implement your own class to match the interface used by SKLearn\n",
    "* More practice with custom data files, Pandas, Numpy, SKLearn, etc.\n",
    "\n",
    "As usual, look for the TODO markers in the notebook below. Please be sure to *remove* the TODO markers as you complete each aspect (i.e. don't leave a TODO that's for something you've actually done, that's poor style because it's confusing to anyone reading your code/documentation/etc.)\n",
    "\n",
    "This lab is intended to be done with a partner (i.e. teams of two); partners will be assigned based on the survey.  You'll have approximately **two weeks** to complete this lab, so be sure to start early and plan properly to get it all done in a timely fashion.  **NOTE: this lab is due after you get back from Fall Break, but I strongly encourage you to finish it and turn it in before you leave!**\n",
    "\n",
    "As with any longer term assignment, it's a chance to train your time management skills.  It is recommended that you complete the implementation of the `LogisticRegression` class (everything in this file up to **Further testing**) during the first week of the lab.  You should be able to run and test the entire `toy` data set.  Week 2 of the lab can then focus on debugging, applying your model to more difficult data sets, adding an extension to your implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Implementation Overview\n",
    "\n",
    "Unlike the previous lab, we are largely leaving the design of your solution up to you.  The only requirement is that you should have an interface that mimics SciKit Learn models.\n",
    "\n",
    "As a reminder, however, we **strongly encourage** you to follow good development practices by doing a top-down design, creating stubs, testing as you go, etc.\n",
    "\n",
    "You will design a class called `LogisticRegression` that will maintain necessary data and add methods to train the model and classify data points.  This has been stubbed out with minimal code below; you will need to add both data members and methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPSC 66 - Machine Learning\n",
    "# Lab 3\n",
    "# Scaffolding by Prof. Ameet Soni & Ben Mitchell\n",
    "# Assignment completed by: Nina Zhuo\n",
    "# Resources used: \n",
    "#   <List any resources you used beyond the ones posted on Blackboard>\n",
    "#   <This can include books, websites, other students, etc.>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started \n",
    "\n",
    "You will write your program implementation in this file.  In addition, you will utilize the following input files located in the `data` directory:\n",
    " \n",
    " * `simple_data.csv` - a toy data set to help debug your algorithm\n",
    " * `mammal_train.csv` and `mammal_test.csv` - pre-split data sets for zoo animals that are either mammals or not\n",
    " * `titanic.csv` - data set of outcomes for individuals on the Titanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries up top.  Feel free to add libraries.  You should not be using any machine learning libraries in your actual Logistic Regression class implementation.  We have included SciKit Learn's `SGDClassifier` below as a debugging tool only.  If implemented correctly, your solution should produce the same weights and predictions as `SGDClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore') #Removes warnings about convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a stopping criteria, you will use 100 iterations instead of calculating weight changes.  Rather than hardcode, we'll set a global variable. **You will probably actually want to change this during debugging.** For example, you can set it to 1 and compare your weights to the correct implementation (`SGDClassifier`) after just one iteration of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXITER = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier\n",
    "\n",
    "As mentioned, we'll use the `SGDClassifier` from SciKit Learn as a baseline comparison tool.  This box imports some data and creates a classifier.  Refer to this when you write more tests later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(eta0=0.1, learning_rate=&#x27;constant&#x27;, loss=&#x27;log&#x27;, max_iter=100,\n",
       "              penalty=&#x27;none&#x27;, shuffle=False, tol=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(eta0=0.1, learning_rate=&#x27;constant&#x27;, loss=&#x27;log&#x27;, max_iter=100,\n",
       "              penalty=&#x27;none&#x27;, shuffle=False, tol=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(eta0=0.1, learning_rate='constant', loss='log', max_iter=100,\n",
       "              penalty='none', shuffle=False, tol=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyX = pd.read_csv('data/simple_data.csv') # open the csv file to see the data\n",
    "toyY = toyX.pop(\"stroke\") #remove the label column\n",
    "\n",
    "alpha = .1 #learning rate, you can change this\n",
    "\n",
    "\"\"\"\n",
    " Create a SGDClassifier with the following settings: use logistic regression for MAX_ITER \n",
    " iterations.  Does not shuffle the data (so we can directly compare results to our algorithm)\n",
    " and use learning rate, alpha, that will not change.  For now, do not impose a penalty on weights. \n",
    "\"\"\"\n",
    "sgd = SGDClassifier(loss='log', max_iter=MAXITER,shuffle=False, tol=None, \\\n",
    "                    penalty='none', learning_rate='constant', eta0 = alpha)\n",
    "sgd.fit(toyX,toyY) # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided Helper Functions\n",
    "\n",
    "These are helper functions provided for you, but feel free to add to or modify them if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printWeights(features, weights):\n",
    "    \"\"\"\n",
    "    Pretty-print the model weights.\n",
    "    features contains the name of each feature and weights is an array type of the same length.\n",
    "    You may need to modify this function depending on your implementation\n",
    "    \"\"\"\n",
    "    if \"bias\" not in features:\n",
    "        print(\"Assuming the last weight is bias term; modify the printWeights function if this is not true\")\n",
    "        features = list(features)+[\"bias\"]\n",
    "    if len(weights) != len(features):\n",
    "        print(\"ERROR: printWeights() called with non-matching feature and weight vector lengths\")\n",
    "        return\n",
    "    print(\"\\t%30s %10s\" % (\"Feature\", \"Weight\"))\n",
    "    for i in range(len(features)):\n",
    "        print(\"\\t%30s %10.3f\" % (features[i], weights[i]))\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    You will get overflow problems when calculating exponentials if \n",
    "    your feature values are too large.  This function adjusts all values to be\n",
    "    in the range of 0 to 1 for each column.\n",
    "    \"\"\"\n",
    "    \n",
    " #  assert (X.min() != X.max()) # make sure the range is non-0 (otherwise, we'll end up with NaNs)\n",
    "        \n",
    "    X = X - X.min() # shift range to start at 0\n",
    "    normalizedX = X/X.max() # divide by possible range of values so max is now 1\n",
    "    return normalizedX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the output of SGDClassify\n",
    "\n",
    "The cell below shows how we can get the parameter values out of the SKLearn model once it's been trained (_note that the training happened several cells prior to this one_).  It also shows the accuracy of the model, which works just like it does with other SKLearn models.\n",
    "\n",
    "Note that we evaluate accuracy on the training data here, since we never split this set; this is fine for the purposes of testing the correctness of our training algorithm, but be careful not to do this when testing on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from SKlearn SGDClassifier on toy data: 1.00\n",
      "\n",
      "Coefficients and intercept for SKlearn SGDClassifier: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                           age      1.967\n",
      "\t                    cholestrol     -1.888\n",
      "\t                          bias     -4.422\n"
     ]
    }
   ],
   "source": [
    "accuracy = sgd.score(toyX, toyY) # note that we're testing on the training data here...\n",
    "print(\"Accuracy from SKlearn SGDClassifier on toy data: %.2f\\n\" % (accuracy))\n",
    "\n",
    "print(\"Coefficients and intercept for SKlearn SGDClassifier: \")\n",
    "weights = list(sgd.coef_[0]) + list(sgd.intercept_) #sklearn stores these weights separately, so lets combine them into one list\n",
    "printWeights(toyX.columns,  weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## LogisticRegression class\n",
    "\n",
    "Define the class and methods here.  Practice incremental development.  If you prefer to code in a native Python environment instead of Jupyter, you can convert this to a Python file (see instructions [here](https://jupyterlab.readthedocs.io/en/stable/user/export.html)).  If you do so, replace the code below with an import statement.  For example, if you name the class definition file `LogRegressionFile.py`, then import everything using \n",
    "\n",
    "```from LogRegFile import *```\n",
    "\n",
    "You will still need to run the tests and provide analysis in this file.\n",
    "\n",
    "Begin by defining the constructor.  Add any data members you want to keep track of.  At a minimum, you should store the weights as well as the parameters given below (learning rate, the names of all features, and the class labels).  You can assume a binary classification task.  Remember to rerun this code block if you change any of the data members. \n",
    "\n",
    "\n",
    "Also, remember that because Python is a dynamically typed language, **an object's type is that of the class _when the object was instanciated._**  In _particular_, if you construct and object and then subsequently add things to the class definition (which is actually overriding the original class definition with a new one that extends the old one but uses the same name), your object will not have those newly added things.  You need to create a new object after executing the new class definition before things will work.\n",
    "\n",
    "The example code below separates the class definition into multiple cells to encourage modular development, but this means that you'll need to move/copy/re-run the testing example given below the initial class definition if you want the object `lr` to actually contain your later additions (we've actually done the \"copy\" option for you in the scaffolding, but it's still a potential issue to be aware of as you modify and run things).\n",
    "\n",
    "Note: in the reference implementation, we assume the first item in the list `label` is the negative class and the second value is the positive.  This is arbitrary. If you flip this, you will get the same accuracy but potentially weights with the opposite magnitude (i.e., all the weights will be negated).  You can assume that all features are numeric for now.  You can also ignore `lmbda` - this is for a possible extension suggested at the bottom of the write up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, features, labels, alpha, lmbda = 0):\n",
    "        \"\"\"\n",
    "        Constructor.  Create class variables here.  At a minimum, you'll need\n",
    "        to store the learning rate (alpha) and weights.  You may also choose to add meta data\n",
    "        e.g., feature names, labels, etc. depending on your needs below\n",
    "        \"\"\"\n",
    "        self.alpha = alpha # learning rate        \n",
    "        self.features = np.array(features) # names of all features\n",
    "        self.labels = np.array(labels) # class labels\n",
    "        self.lmbda = lmbda\n",
    "        self.w = [] # weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.LogisticRegression object at 0x7f3c84e07e20>\n",
      "features =  ['age' 'cholestrol']\n",
      "labels =  ['Neg' 'Pos']\n",
      "alpha =  0.1\n",
      "lmbda =  0\n",
      "w =  []\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(toyX.columns, toyY.unique(), alpha) # create a model similar to the SGDClassifier above\n",
    "\n",
    "print(lr)\n",
    "print(\"features = \", lr.features)\n",
    "print(\"labels = \", lr.labels)\n",
    "print(\"alpha = \", lr.alpha)\n",
    "print(\"lmbda = \", lr.lmbda)\n",
    "print(\"w = \", lr.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Training algorithm\n",
    "\n",
    "Your class should have a method `fit(X,y)` that trains the model weights.  It takes in training examples `X` and their labels `Y` which can be any array-like object you choose (Pandas data frames, numpy arrays, Python lists, etc).  It does not return anything.  It will use **stochastic gradient descent** for its implementation.  Recall the math for gradient descent from class, as well as the fact that the S in **S**GD means we will update weights after each example seen.  As a reminder of the equations we covered:\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "\n",
    "for each weight $w_j$ in the weight vector, use **one** training example with index $i$ (then repeat with a different $i$):\n",
    "\n",
    "$$ w_j \\leftarrow w_j - \\alpha \\left( P(y=1 | \\vec{x}^i) - y^i \\right) \\vec{x}_j^i $$\n",
    "\n",
    "where\n",
    "\n",
    "$$P(y=1 | \\vec{x}^i) = \\frac{1}{1 + e^{- \\left( \\vec{w} \\cdot \\vec{x}^i \\right) }} $$\n",
    "\n",
    "\n",
    "Rember that $\\vec{u}\\cdot\\vec{v}$ is the same as $\\vec{u}^\\top\\vec{v}$; it's just two different notations for the same operation (this is important to know as different references may use different notation).\n",
    "\n",
    "***\n",
    "Here is some pseudocode for the algorithm:\n",
    "\n",
    "```\n",
    "initialize weights to 0\n",
    "while not converged:\n",
    "    initialize gradient to 0 for each feature (g = [0,...,0])\n",
    "    for each training example xi:\n",
    "        p = prob(w, xi) //predict probabilty of positive under current model\n",
    "        error = p-yi // calculate error in prediction\n",
    "        for each feature j:\n",
    "            g[j] = error*xi[j]\n",
    "            w[j] -= alpha*g[j] //update weights\n",
    "```\n",
    "\n",
    "***\n",
    "For simplicity, use a maximum number of iterations of the outer loop as the convergence criteria (the number should be set by the constant `MAXITER`, defined above).\n",
    "\n",
    "Note that there must be a weight for the bias/intercept term.  To accommodate, you'll need to either add a fake value of 1 to your training set examples to create an extra feature, or explicitely add a bias term (and associated weight) to your class data members.  The extra 1 to make the vector lengths match _is not_ built-in to your training set. \n",
    "\n",
    "Make sure not to modify the original input data when you do this(i.e., you may need to create a copy of `X`).  For example, if you decide to make `X` a Pandas dataframe, you can create a bias \"feature\" like so:\n",
    "\n",
    "```\n",
    "copyX = X.copy() // makes a \"deep\" copy of a pandas dataframe\n",
    "copyX[\"bias\"] = 1 // adds a column \"bias\" and sets each row's value for that new colum to 1 (modifies copyX but not X)\n",
    "```\n",
    "\n",
    "You will probably find the [Numpy `dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) function useful as well as the [`math.exp()`](https://www.w3schools.com/python/ref_math_exp.asp) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is effectively adding an extra method to the existing class\n",
    "# (though technically it's actually a new class inheriting from the previous one,\n",
    "# but with the same name so it hides the old one)\n",
    "class LogisticRegression(LogisticRegression):\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        train the model weights.  Initialize the weights to 0 first and use gradient ascent \n",
    "        using the training examples {X,Y}\n",
    "        \n",
    "        X = training examples\n",
    "        Y = array-like object with labels\n",
    "        \"\"\"        \n",
    "        \n",
    "        # create deep copy of X and add bias column\n",
    "        copyX = X.copy()\n",
    "        copyX[\"bias\"] = 1\n",
    "        \n",
    "        # create copy of Y to index into\n",
    "        Y = Y.tolist()\n",
    "                \n",
    "        # initialize weights to zero\n",
    "        self.w = [0]*(len(self.features) + 1)\n",
    "        \n",
    "        # set convergence criteria\n",
    "        iters = 0\n",
    "        while iters < MAXITER:\n",
    "            \n",
    "            # initialize gradient to 0 for each feature\n",
    "            g = [0]*len(self.w)\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                \n",
    "                # set xi\n",
    "                xi = copyX.iloc[i].to_numpy()\n",
    "                \n",
    "                # calculate probability and error for each example\n",
    "                p = self.prob(xi)\n",
    "                                \n",
    "                if Y[i] == self.labels[0]:\n",
    "                    error = p\n",
    "                else:\n",
    "                    error = p - 1\n",
    "\n",
    "                # updates weights\n",
    "                for j in range(len(self.w)):\n",
    "                    g[j] = error*xi[j]\n",
    "                    self.w[j] -= self.alpha*g[j]\n",
    "            \n",
    "            iters += 1\n",
    "                                    \n",
    "    def prob(self, xi):\n",
    "        if (len(self.w)) > len(xi):\n",
    "            xi = np.append(xi, 1)    \n",
    "    \n",
    "        p = -1*np.dot(self.w, xi)\n",
    "        p = np.exp(p)\n",
    "        p = 1/(1+p)\n",
    "            \n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests for training\n",
    "\n",
    "Let's compare the weights of your classifier to the SciKit Learn implementation.  Your implementation should match the `SGDClassifier` assuming you go for the same number of iterations.  If you have the opposite magnitude that is okay. Be sure and try just a few iterations at first to see where you are deviating; you can calculate the values by hand to make sure your math is correct.  Another suggestion is create an even smaller data set with e.g., 2 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients and intercept for SKlearn SGDClassifier: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                           age      1.967\n",
      "\t                    cholestrol     -1.888\n",
      "\t                          bias     -4.422\n",
      "\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                           age      1.967\n",
      "\t                    cholestrol     -1.888\n",
      "\t                          bias     -4.422\n"
     ]
    }
   ],
   "source": [
    "#TODO: add further testing code here to help debug your training algorithm\n",
    "lr = LogisticRegression(toyX.columns, toyY.unique(),alpha) # create a model similar to the SGDClassifier above\n",
    "lr.fit(toyX,toyY) #fit the same data as the SGDClassifier\n",
    "\n",
    "print(\"Coefficients and intercept for SKlearn SGDClassifier: \")\n",
    "weights = list(sgd.coef_[0]) + list(sgd.intercept_) #sklearn stores these weights separately, so lets combine them into one list\n",
    "printWeights(toyX.columns,  weights)\n",
    "\n",
    "print()\n",
    "print(\"Coefficients and intercept for Logistic Regression: \")\n",
    "printWeights(toyX.columns,  lr.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Prediction/Inference \n",
    "\n",
    "You will implement two different functions to provide a prediction for a novel example: `predict_prob` returns the probability of the positive label, while `predict` will return a discrete label.\n",
    "\n",
    "***\n",
    "**Predicting a probability**\n",
    "To predict the probability of a positive label, use the equation from class: \n",
    "\n",
    "$$P(y=1|\\vec{x}) = \\frac{1}{1+e^{-\\left(\\vec{w}\\cdot\\vec{x}\\right)}}$$\n",
    "\n",
    "_Note that this is closely related to a part of the SGD algorithm from above; consider ways to make your implementation modular to avoid code replication._\n",
    "\n",
    "***\n",
    "**Predicting a label**\n",
    "To produce a discrete prediction, you can either compare the positive vs negative probabilities and pick the larger (HINT: they sum to 1, so you just need to know if the probability of a positive is greater than 0.5).  Another common technique is just to see if $w^Tx$ is greater than 0: a value of greater than 0 yields a probability score greater than 0.5, and a negative value gives a probability of less than 0.5.  \n",
    "\n",
    "_Note again that you can and should use your `predict_prob` function here to avoid code replication!_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LogisticRegression):\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        \"\"\"\n",
    "        Returns a 2D array of size [num_examples, num_classes] containing the probability \n",
    "        of each prediction.  Since logistic regression is a binary classifier, there should only be\n",
    "        two columns.  That is, give the probability that the model would assign the two possible labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        # set up 2D array with n subarrays with length m \n",
    "        # n = num_examples, m = num_classes\n",
    "        n, m = (len(X), len(self.labels)) \n",
    "        arr = [[0 for i in range(m)] for j in range(n)] \n",
    "        \n",
    "        \n",
    "                    \n",
    "        for i in range(n):\n",
    "            # get row\n",
    "            xi = X.iloc[i].tolist()\n",
    "            \n",
    "            # calculate probability\n",
    "            p = self.prob(xi)\n",
    "            \n",
    "            # save probability\n",
    "            arr[i][1] = p\n",
    "            arr[i][0] = 1 - p\n",
    "                    \n",
    "        return arr\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        classify the examples in X.  Returns the predicted label for each example in X (be sure to use the stored\n",
    "        labels array from the constructor instead of just returning 0s and 1s).  The length of \n",
    "        the returned array should be the same as the number of examples in X.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get 2d array of probability of each prediction\n",
    "        prob = self.predict_prob(X)\n",
    "        \n",
    "        # set threshold for binary classification\n",
    "        div = 0.5   \n",
    "        \n",
    "        # initialize array storing predicted labels\n",
    "        arr = [0]*len(X)\n",
    "        \n",
    "        # determines predicted label for each example\n",
    "        for i in range(len(X)):\n",
    "            if prob[i][1] > div:\n",
    "                arr[i] = self.labels[1]\n",
    "            else:\n",
    "                arr[i] = self.labels[0]\n",
    "                        \n",
    "        return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests for prediction\n",
    "Now let's test the implementation on the toy data set to see if matches.  Since are making predictions on the training set and the problem is fully linear, you should get 100% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and Predictions for **SKlearn SGDClassifier**: \n",
      "Accuracy: 100.00%\n",
      "  Prediction        Truth\n",
      "-------------------------\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "\n",
      "Accuracy and Predictions for **Custom Logistic Regression**: \n",
      "Accuracy: 100.00%\n",
      "  Prediction        Truth\n",
      "-------------------------\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n"
     ]
    }
   ],
   "source": [
    "### Tests for SKLearn\n",
    "predictions = sgd.predict(toyX)\n",
    "score = sgd.score(toyX,toyY)\n",
    "print(\"Accuracy and Predictions for **SKlearn SGDClassifier**: \")\n",
    "print(\"Accuracy: %0.2f%%\" % (score*100))\n",
    "\n",
    "print(\"%12s %12s\" % (\"Prediction\", \"Truth\"))\n",
    "print(\"-\"*25)\n",
    "for i in range(len(predictions)):\n",
    "    print(\"%12s %12s\" % (predictions[i], toyY[i]))\n",
    "print()\n",
    "\n",
    "### Tests for your implementation for comparison\n",
    "lr = LogisticRegression(toyX.columns, toyY.unique(),alpha) # create a model similar to the SGDClassifier above\n",
    "lr.fit(toyX,toyY) #fit the same data as the SGDClassifier\n",
    "predictions = lr.predict(toyX)\n",
    "\n",
    "if predictions == None:\n",
    "    print(\"Error: LogisticRegression.predict() not yet implemented\")\n",
    "else: \n",
    "    score = len(toyY[toyY == lr.predict(toyX)])/len(toyY)\n",
    "    print(\"Accuracy and Predictions for **Custom Logistic Regression**: \")\n",
    "    print(\"Accuracy: %0.2f%%\" % (score*100))\n",
    "\n",
    "    print(\"%12s %12s\" % (\"Prediction\", \"Truth\"))\n",
    "    print(\"-\"*25)\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"%12s %12s\" % (predictions[i], toyY[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Further testing: Mammal Prediction Task\n",
    "\n",
    "Let's try some real data.  Test your implementation on the Mammal data set in `data` (`mammal_train.csv` and `mammal_test.csv`).  This is a simplified version of the [zoo animal](https://archive.ics.uci.edu/ml/datasets/Zoo) data set where all animals in category 1 have been given the label `Mammal` and all other categories are `Other`.  The data has already been divided into training and testing sets for you.  We've provided you code to load the data set and normalize the features (make sure they all range from 0 to 1).  You should write code that at a minimum:\n",
    "\n",
    "* trains your classifier\n",
    "* prints out the weights \n",
    "* checks the accuracy on the test set\n",
    "\n",
    "We also recommend comparing your weights and your predictions with the `SGDClassifier` and seeing if the results weights make sense in the real world (i.e., are these features correlated with mammals in real life?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients and intercept for SKlearn SGDClassifier: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                          hair     -2.930\n",
      "\t                      feathers      1.311\n",
      "\t                          eggs      3.691\n",
      "\t                          milk     -4.335\n",
      "\t                      airborne      1.432\n",
      "\t                       aquatic      0.629\n",
      "\t                      predator      0.616\n",
      "\t                       toothed     -0.377\n",
      "\t                      backbone     -0.442\n",
      "\t                      breathes     -0.484\n",
      "\t                      venomous      1.537\n",
      "\t                          fins      0.580\n",
      "\t                          legs      0.507\n",
      "\t                          tail      0.199\n",
      "\t                      domestic      0.174\n",
      "\t                       catsize     -1.975\n",
      "\t                          bias      1.803\n",
      "\n",
      "\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                          hair      2.930\n",
      "\t                      feathers     -1.311\n",
      "\t                          eggs     -3.691\n",
      "\t                          milk      4.335\n",
      "\t                      airborne     -1.432\n",
      "\t                       aquatic     -0.629\n",
      "\t                      predator     -0.616\n",
      "\t                       toothed      0.377\n",
      "\t                      backbone      0.442\n",
      "\t                      breathes      0.484\n",
      "\t                      venomous     -1.537\n",
      "\t                          fins     -0.580\n",
      "\t                          legs     -0.507\n",
      "\t                          tail     -0.199\n",
      "\t                      domestic     -0.174\n",
      "\t                       catsize      1.975\n",
      "\t                          bias     -1.803\n",
      "\n",
      "\n",
      "Accuracy and Predictions on Testing Set: \n",
      "Accuracy: 100.00%\n",
      "  Prediction        Truth\n",
      "-------------------------\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "      Mammal       Mammal\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "      Mammal       Mammal\n",
      "      Mammal       Mammal\n",
      "       Other        Other\n",
      "       Other        Other\n"
     ]
    }
   ],
   "source": [
    "# load the data sets, train your model and print out the weights and predictions.  \n",
    "# Play around with alpha; you should be able to get 100% Accuracy\n",
    "\n",
    "mammal_train = pd.read_csv('data/mammal_train.csv')\n",
    "mammal_test = pd.read_csv('data/mammal_test.csv')\n",
    "\n",
    "# Some minimal data processing.  Let's shuffle the examples so they don't appear in order\n",
    "# Then, we should normalize the legs feature so all values are between 0 and 1\n",
    "mammal_train = mammal_train.sample(frac=1).reset_index(drop=True)\n",
    "mammal_test = mammal_test.sample(frac=1).reset_index(drop=True)\n",
    "mammal_train[\"legs\"] /= 8 #all features are normalized to be between 0 and 1 except for legs, so let's normalize it \n",
    "mammal_test[\"legs\"] /= 8 #all features are normalized to be between 0 and 1 except for legs, so let's normalize it \n",
    "\n",
    "# separate out labels from training data\n",
    "ytrain = mammal_train.pop(\"animalType\")\n",
    "ytest = mammal_test.pop(\"animalType\")\n",
    "\n",
    "# set up sgd model\n",
    "sgd.fit(mammal_train, ytrain) # train the model\n",
    "print(\"Coefficients and intercept for SKlearn SGDClassifier: \")\n",
    "weights = list(sgd.coef_[0]) + list(sgd.intercept_) #sklearn stores these weights separately, so lets combine them into one list\n",
    "printWeights(mammal_train.columns,  weights)\n",
    "print()\n",
    "\n",
    "# compare to implemented lr model\n",
    "lr = LogisticRegression(mammal_train.columns, ytrain.unique(),alpha)\n",
    "lr.fit(mammal_train, ytrain) \n",
    "print()\n",
    "print(\"Coefficients and intercept for Logistic Regression: \")\n",
    "printWeights(mammal_train.columns,  lr.w)\n",
    "print()\n",
    "\n",
    "# run lr model on training set\n",
    "predictions = lr.predict(mammal_test)\n",
    "if predictions == None:\n",
    "    print(\"Error: LogisticRegression.predict() not yet implemented\")\n",
    "else: \n",
    "    score = len(ytest[ytest == lr.predict(mammal_test)])/len(ytest)\n",
    "    print(\"\\nAccuracy and Predictions on Testing Set: \")\n",
    "    print(\"Accuracy: %0.2f%%\" % (score*100))\n",
    "\n",
    "    print(\"%12s %12s\" % (\"Prediction\", \"Truth\"))\n",
    "    print(\"-\"*25)\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"%12s %12s\" % (predictions[i], ytest[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Further testing: Titanic Data Set\n",
    "\n",
    "Now, we'll try a harder task: the titanic data set in `data/titanic.csv`.\n",
    "\n",
    "The dataset in question is based on the passenger list from the Titanic; each passenger has a record, with some known information, along with whether or not that passenger survived the disaster when the ship sank.  The standard supervised learning task for this data set is to predict whether a given passenger survived given the rest of their information.   If who survived was random, this shouldn't work (we wouldn't be able to do better than chance), but if it's not random then we might be able to build a successful classifier, and ideally even understand what kind of traits the survivors had in common.\n",
    "\n",
    "This data was first posted on Kaggle: https://www.kaggle.com/c/titanic/data, and that site also has a description of what the columns mean.  Note however t?hat the version of the data we've provided you has been cleaned and pre-processed to remove missing values, non-numeric features, etc.\n",
    "\n",
    "Below write some test code and see if you can use your implementation to test the robustness of your implementation.  Be sure to do a proper train/test evaluation. Does changing the learning rate impact performance?  Do the feature weights make sense for this task?  Do we get interesting results if we examine the types of errors?   **Write 2-3 sentences describing any interesting findings for this data set** in the \"Analysis\" area below the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestAndTrain(data, trainFrac =0.7, seed =0):\n",
    "    # let's shuffle this and split it into train and test sets:\n",
    "    shuffled = data.sample(frac=1, random_state=seed) # randomly re-order the examples\n",
    "        # note: the 'frac=1' means use all the examples; also note this creates a new \"view\", \n",
    "        # it doesn't do a deep copy, so the row index values in `shuffled` will be out of order\n",
    "    trainCount = int(len(data) * trainFrac) # figure out how many examples we want in each set\n",
    "    train = shuffled[:trainCount] # slice out the examples we'll use for training (again, creates a view)\n",
    "    test = shuffled[trainCount:] # use the remainder for testing\n",
    "\n",
    "    return [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients and intercept for SKlearn SGDClassifier: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                      Survived    158.316\n",
      "\t                        Pclass    -26.654\n",
      "\t                           Sex     49.212\n",
      "\t                         SibSp    -10.660\n",
      "\t                         Parch     -2.991\n",
      "\t                          Fare     -0.029\n",
      "\t                          bias    -19.944\n",
      "\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "Assuming the last weight is bias term; modify the printWeights function if this is not true\n",
      "\t                       Feature     Weight\n",
      "\t                      Survived    166.658\n",
      "\t                        Pclass    -27.186\n",
      "\t                           Sex     54.009\n",
      "\t                         SibSp    -12.501\n",
      "\t                         Parch     -4.460\n",
      "\t                          Fare      0.030\n",
      "\t                          bias    -19.468\n",
      "\n",
      "\n",
      "Accuracy and Predictions on Testing Set: 99.63%\n"
     ]
    }
   ],
   "source": [
    "# divide data into test and training sets\n",
    "titanicData = pd.read_csv('data/titanic.csv')\n",
    "sets = getTestAndTrain(titanicData)\n",
    "\n",
    "# set up X and Y for testing and training sets\n",
    "trainX = sets[0]\n",
    "normalize(trainX)\n",
    "trainY = trainX[\"Survived\"]\n",
    "trainX.drop(columns=[\"Survived\"])\n",
    "\n",
    "testX = sets[1]\n",
    "normalize(testX)\n",
    "testY = testX[\"Survived\"]\n",
    "testX.drop(columns=[\"Survived\"])\n",
    "\n",
    "# set up sgd model\n",
    "print(\"Coefficients and intercept for SKlearn SGDClassifier: \")\n",
    "sgd.fit(trainX, trainY) # train the model\n",
    "weights = list(sgd.coef_[0]) + list(sgd.intercept_) #sklearn stores these weights separately, so lets combine them into one list\n",
    "printWeights(trainX.columns,  weights)\n",
    "print()\n",
    "\n",
    "# compare to implemented lr model\n",
    "print(\"Coefficients and intercept for Logistic Regression: \")\n",
    "lr = LogisticRegression(trainX.columns, trainY.unique(), alpha)\n",
    "lr.fit(trainX, trainY)\n",
    "printWeights(trainX.columns, lr.w)\n",
    "print()\n",
    "\n",
    "# run lr model on training set\n",
    "predictions = lr.predict(testX)\n",
    "if predictions == None:\n",
    "    print(\"Error: LogisticRegression.predict() not yet implemented\")\n",
    "else: \n",
    "    score = len(testY[testY == lr.predict(testX)])/len(testY)\n",
    "    print(\"\\nAccuracy and Predictions on Testing Set: %0.2f%%\" % (score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Unsurprisingly, increasing the value value of alpha decreases the overall accuracy of the model because the weights are taking larger steps away from the error, which gets us to the model faster, but sacrifices greater accuracy. Looking at the model, it makes sense that the sex of the passenger is weighted the highest in almost every iteration because when the life boats came out, women and children were prioritized over male passengers. When increasing the learning rate however, I was surprised to see that a higher fare contributed to greater likelihood of survival, when the opposite is true for a lower learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extensions\n",
    "\n",
    "**You are required to pick and implement one of these extensions**.  It is up to you which one you choose - they are varying degrees of difficulty involved.  Feel free to implement more than one!\n",
    "\n",
    "### Multiclass prediction\n",
    "\n",
    " Logistic Regression is designed for binary classification tasks.  However, we can design a fairly simple extension for multiclass prediction (as most machine learning packages do) known as 1-one-vs all prediction.  Review [this page](http://mlwiki.org/index.php/One-vs-All_Classification) for a description.  You can use the following pseudocode to structure your design:\n",
    "``` \n",
    "    given training examples X,y\n",
    "    for each class k\n",
    "       create y_k where all instances of class k are positive and all other instances are negative\n",
    "       train a logistic regression model on X,y_k --> model_k\n",
    "    \n",
    "    given a test example x_test\n",
    "    run x_test on each model_k to give prob_k (probability of positive under model_k)\n",
    "    return the k with the highest prob_k as the prediction\n",
    "```\n",
    "\n",
    "Pick a data set (we recommend the digits task from Lab 2B or the zoo data set provided in `data`).   The major deliverable is the code to implement this extension; no analysis is necessary except to explain what data set you used. \n",
    "\n",
    "### Penalize weights\n",
    "\n",
    "Add an L2 penalty term to the training algorithm.  This requires minimal modification to your code.  We have already provided the option of specifying the $\\lambda$ as part of the constructor for `LogisticRegression`.  Modify your `LogisticRegression` code above to use that `lmbda` and change your stochastic gradient descent algorithm to incorporate this into weight updates.  \n",
    "\n",
    "Since this is a bit easier to implement, you'll need to **provide a few sentences of analysis**.  In particular, we want you to try out a few different $\\lambda$ values and describe how it impacts performance as well as the weights (do they change significantly in one way or another?).  Does this align with our discussion on regularization in lecture?  Pick a data set for your analysis (probably titanic or some other difficult problem).  \n",
    "\n",
    "### Multinomial Features and Data Cleanup\n",
    "\n",
    "Download and clean up your own data set that uses multinomial features (discrete with more than 2 values).  Note: this option may be a bit trickier and open-ended than the other options, but is good practice for the final project.  \n",
    "\n",
    "Test out the on your classifier implementation.  Your could should include non-trivial preprocessing and testing out different hyperparameters (e.g,. $\\alpha$, `MAXITERS`).  At a minimum, we want to see how you adapt your model to work with \"messy\" data sets as a preparation for the final project.  You may way want to think about the Tennis data set (`data/tennis.csv`) as a thought experiment.  How can you use this data set in logistic regression?  In your analysis, include any clean up steps you performed and why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModels(X, Y, classes):\n",
    "\n",
    "        # stores all linear regression models for each class k\n",
    "        models = [None]*len(classes)\n",
    "\n",
    "        for k in range(len(classes)):\n",
    "            # creates y_k where all instances of class k are > 0 and all other instances are < 0\n",
    "            y_k = np.zeros(len(Y))\n",
    "            for i in range(len(Y)):\n",
    "                if Y.iloc[i] == classes[k]:\n",
    "                    y_k[i] = 1\n",
    "                    \n",
    "            # train logistic regression model to find instances that are or not k\n",
    "            model_k = LogisticRegression(X.columns, np.unique(y_k), alpha)            \n",
    "            model_k.fit(X, y_k)\n",
    "            models[k] = model_k\n",
    "            \n",
    "        return models\n",
    "\n",
    "def predictLabel(classes, models, query):\n",
    "            \n",
    "        maxProb = 0\n",
    "        label = 0\n",
    "        \n",
    "        # run query on each model_k\n",
    "        for k in range(len(models)):\n",
    "            model_k = models[k]\n",
    "            probability = model_k.prob(query)            \n",
    "            if probability > maxProb:\n",
    "                maxProb = probability\n",
    "                label = classes[k]\n",
    "    \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy and Predictions on Testing Set: 90.32%\n"
     ]
    }
   ],
   "source": [
    "# divide data into test and training sets\n",
    "zooData = pd.read_csv('data/zoo.csv')\n",
    "sets = getTestAndTrain(zooData)\n",
    "print\n",
    "\n",
    "# set up X and Y for testing and training sets\n",
    "trainX = sets[0]\n",
    "normalize(trainX)\n",
    "trainY = trainX[\"animalType\"]\n",
    "trainX.drop(columns=[\"animalType\"])\n",
    "\n",
    "testX = sets[1]\n",
    "normalize(testX)\n",
    "testY = testX[\"animalType\"]\n",
    "testX.drop(columns=[\"animalType\"])\n",
    "\n",
    "# get classes and train model\n",
    "classes = trainY.unique()\n",
    "models = getModels(trainX, trainY, classes)\n",
    "\n",
    "# check accuracy\n",
    "predict = []\n",
    "for i in range(len(testX)):\n",
    "    prediction = predictLabel(classes, models, testX.iloc[i])\n",
    "    predict.append(prediction)\n",
    "    \n",
    "score = len(testY[testY == predict])/len(testY)\n",
    "print(\"\\nAccuracy and Predictions on Testing Set: %0.2f%%\" % (score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Analysis\n",
    "\n",
    "Clearly state what extension you did, what you learned, and a bit of analysis that is suggested as part of the descriptions above.  You shouldn't need more than 5 or 6 sentences of analysis (and probably less).\n",
    "\n",
    "I did the Multiclass prediction extension on the zoo dataset in data/zoo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
